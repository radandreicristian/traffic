################################## Experiment Configuration #######################################

project_name:
##################################### Data Configuration ##########################################

# The name of the dataset - Either "metr_la" or "pems_bay" (wip)
dataset: metr_la

in_memory: True
# Random walk (rw) or symmetric gcn norm (gcn)
# Todo - Read more on this
data_norm: rw

# Weights of self-loops (for the weighted adjacency matrix)
self_loop_weight: 1.

valid_percentage: .2

test_percentage: .2

sample_train: True

sample_train_factor: 1.

train_batch_size: 128
test_batch_size: 128
##################################### Model Configuration ##########################################

# 'gdr' (graph diffusion recurrent net) or 'lgdr' (latent graph diffusion recurrent net)
model_type: gdr

# Size of the hidden feature vectors in the model (int)
d_hidden: 32

# Adds a fully-connected layer at the beginning of the encoder (bool) {use_mlp}
use_mlp_in: True

# Adds an extra fully-connected layer at the end of the decoder (bool)
use_mlp_out: True

# Dropout percent of the input (float, <1.)
p_dropout_in: .4

# Dropout rate of the model (float, <1.)
p_dropout_model: .4

# Batch normalization (bool) {batch_norm}
use_batch_norm: True

# Optimizer (str, sgd/rmsprop/adam/adamax)
optimizer: adam

# Learning rate of the optimizer
lr: 2e-2

# Weight decay
weight_decay: 5e-3

# Number of epochs for training
n_epochs: 25

# The factor of matrix A - Used only in C(ontinuous)GNN  src/CGNN.py (float <= 1.)
# Todo - Read more on this
alpha: 1.

# Alpha dimension, scalar or vector (str, sc/vc)
alpha_dim: sc

# Apply sigmoid before multiplying by alpha (bool)
alpha_sigmoid: True

# Beta dimension, scalar or vector (str, sc/vc)
beta_dim: sc

# The ODE block - currently supports only "attention" (str, attention)
block: attention

# The ODE function - currently supports only "transformer"
function: transformer

# TODO add source? (bool)
add_source: True

###################################### ODE Configuration ###########################################

# End time of the ODE integrator (float)
time: 3.

# Augment: double the length of feature vectors by appending zeros to stabilize ODE learning (bool)
use_augmentation: True

# Numerical solver (str, dopri5/euler/rk4/midpoint) {method}
solver: dopri5

# Step size of the ODE (float)
step_size: 1.

# Maximum number of ODE iterations (int)
max_iters: 200

# Use adjoint ODE method to reduce memory (bool)
# Todo - Read more on this
adjoint: True

# What solver to use when using adjoint method (str, adaptive_heun/dopri5/euler/rk4/midpoint)
adjoint_solver: rk4
adjoint_step_size: 1.

# Tolerance scale - Multiplier for absolute and relative tolerance (float)
tol_scale: 10.
tol_scale_adjoint: 10.

# Number of ODE blocks to run
n_ode_blocks: 1

# Maximum number of function evaluations in each epoch/batch? (int) {<max_nfe>}
max_func_evals: 500

# Use early stopping of the ODE integrator on inference (bool) {no_early}
early_stopping_integrator: True

# Multiplier for T used to evaluate best model (int) {earlystopxt}
# Todo - Read more on this
early_stop_multiplier: 3

# Maximum number of steps for dopri5early test integrator, used if OOM
max_test_steps: 100

##################################### Attention Configuration #####################################
# Slope for the x<0 part of the LeakyReLU
leaky_relu_slope: .2

# Dropout of attention weights
attention_dropout: .3

# Number of attention heads (int) {heads}
n_heads: 8

# Attention normalization index (int, 0/1) - 0: by rows, 1: by cols
attention_norm_idx: 0

# Projection size before applying attention (int)
d_hidden_attention: 32

# Apply a feature transformation xW to the ODE (bool)
mix_features: True

# Multiply attention scores by edge weights before softmax
reweight_attention: True

# Type of attention (str, scaled_dot/cosine/pearson/exp_kernel, currently only scaled_dot)
attention_type: scaled_dot

# Use squareplus instead of softmax: https://arxiv.org/pdf/2112.11687.pdf
square_plus: False

################################### Regularization Configuration ###################################
jacobian_norm2: False # int_t ||df/dx||_F^2
total_derivative: False # int_t ||df/dt||^2
kinetic_energy: False # int_t ||f||_2^2
directional_derivative: False # int_t ||(df/dx)^T f||^2


##################################### Traffic Task Configuration ###################################
n_previous_steps: 12

n_future_steps: 12

n_features: 12
##################################### Rewiring Configuration ######################################
# TODO

##################################### Beltrami Configuration ######################################
use_beltrami: False
# TODO

# Random Notes:
# Attention - changes through the diffusion process
# Rewiring - Happens at the start of the epoch based on features at t=0